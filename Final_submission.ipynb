{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled13.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Rjmy-D8tsMw9","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from scipy.sparse import coo_matrix, hstack\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","import string\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import Perceptron\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHkWn5tAsYkz","colab_type":"code","colab":{}},"source":["###Reading the training data\n","l1=[]\n","l2=[]\n","with open('train_tweets.txt',encoding ='utf-8') as f:\n","    for line in f:\n","        line = line.split(maxsplit=1)\n","        l1.append(line[0])\n","        l2.append(line[1])\n","        \n","        \n","df = pd.DataFrame([l1,l2],index=['user_id','tweet']).T\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYSoFi52seva","colab_type":"code","colab":{}},"source":["##Preprocessing of tweets which are having length lower than 5 and higher than 30\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","def preprocessing(text):\n","    if (len(text.split())>5 and len(text.split())<30):\n","       tokens = word_tokenize(text)\n","        tokens = [w.lower() for w in tokens]\n","        # remove punctuation from each word\n","        import string\n","        table = str.maketrans('', '', string.punctuation)\n","        stripped = [w.translate(table) for w in tokens]\n","        # remove remaining tokens that are not alphabetic\n","        words = [word for word in stripped if word.isalpha()]\n","        # filter out stop words\n","        from nltk.corpus import stopwords\n","        stop_words = set(stopwords.words('english'))\n","        words = [w for w in words if not w in stop_words]\n","        porter = PorterStemmer()\n","        stemmed = [porter.stem(word) for word in tokens]\n","\n","        return \" \".join(stemmed)\n","    else:\n","        return np.nan"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UBe3RDj4sYhj","colab_type":"code","colab":{}},"source":["df['preprocessed_tweet'] = df['tweet'].apply(lambda x:preprocessing(x))\n","df= df.dropna(axis=0)\n","df =df.groupby('user_id').filter(lambda x : len(x)>20)## droping the user_id and tweets who have less than 20 tweets\n","df.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BaBPWn4esYe5","colab_type":"code","colab":{}},"source":["##reading the test data\n","l3=[]\n","\n","with open('test_tweets_unlabeled.txt',encoding ='utf-8') as f:\n","    for line in f:\n","        l3.append(line)\n","testdf = pd.DataFrame([l3],index=['tweet']).T"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rg_3k_zZsYcU","colab_type":"code","colab":{}},"source":["###Genration of counts for numeric features\n","%%time\n","df['char_count'] = df['tweet'].apply(len)\n","df['word_count'] = df['tweet'].apply(lambda x: len(x.split()))\n","df['word_density'] = df['char_count'] / (df['word_count']+1)\n","df['punctuation_count'] = df['tweet'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n","df['title_word_count'] = df['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n","df['upper_case_word_count'] = df['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n","\n","testdf['char_count'] = testdf['tweet'].apply(len)\n","testdf['word_count'] = testdf['tweet'].apply(lambda x: len(x.split()))\n","testdf['word_density'] = testdf['char_count'] / (testdf['word_count']+1)\n","testdf['punctuation_count'] = testdf['tweet'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n","testdf['title_word_count'] = testdf['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n","testdf['upper_case_word_count'] = testdf['tweet'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jesMpXfbsYZ7","colab_type":"code","colab":{}},"source":["##Functions to check presence of urls, handles and RTs\n","def checkHandle(text):\n","    if '@handle' in text.split():\n","        return 1\n","    else:\n","        return 0\n","\n","\n","def checkRT(text):\n","    try:\n","        if 'RT' in text.split():\n","            return 1\n","        else:\n","            return 0\n","        \n","    except:\n","        return 0\n","\n","def checkURL(text):\n","    try:    \n","        if 'http' in text:\n","            return 1\n","        else:\n","            return 0\n","    \n","    except:\n","        return 0\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5H-LDIxisYXo","colab_type":"code","colab":{}},"source":["df['@handle'] = df['tweet'].apply(lambda x : checkHandle(x))\n","testdf['@handle'] = testdf['tweet'].apply(lambda x : checkHandle(x))\n","df['RT'] = df['tweet'].apply(lambda x : checkRT(x))\n","testdf['RT'] = testdf['tweet'].apply(lambda x : checkRT(x))\n","testdf['tweet_length'] = testdf['tweet'].apply(lambda x:checkLength(x))\n","df['tweet_length'] = df['tweet'].apply(lambda x:checkLength(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p7GuIQ3YsYVW","colab_type":"code","colab":{}},"source":["##Normalizing the counts of the data frames\n","x_train = df[df.columns[3:55]]\n","train_norm = x_train[x_train.columns[0:16]]\n","from sklearn import preprocessing\n","std_scale = preprocessing.StandardScaler().fit(train_norm)\n","x_train_norm = std_scale.transform(train_norm)\n","training_norm_col = pd.DataFrame(x_train_norm, index=train_norm.index, columns=train_norm.columns) \n","x_train.update(training_norm_col)\n","print (x_train.head())\n","\n","x_test = testdf[testdf.columns[1:55]]\n","test_norm = x_test[x_test.columns[:16]]\n","x_test_norm = std_scale.transform(test_norm)\n","testing_norm_col = pd.DataFrame(x_test_norm, index=test_norm.index, columns=test_norm.columns) \n","x_test.update(testing_norm_col)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdToAzKIsYSj","colab_type":"code","colab":{}},"source":["## creation of count vectors\n","word_vectorizer = CountVectorizer(analyzer='word',encoding = 'utf-8',stop_words='english', max_features=7000)\n","char_vectorizer = CountVectorizer(encoding='utf-8',analyzer='char',max_features=500)\n","word_vectorizer.fit(df['preprocessed_tweet'])\n","char_vectorizer.fit(df['tweet'])\n","xtrain1word = word_vectorizer.transform(x_train)\n","xtrain1char = char_vectorizer.transform(x_train)\n","x_train1 = hstack([xtrain1word,xtrain1char])\n","\n","y_train = df.user_id\n","##Count vectors for test\n","xtestword = word_vectorizer.transform(testdf['tweet'])\n","xtestcount = char_vectorizer.transform(testdf['tweet'])\n","x_test = hstack([xtestword,xtestcount])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RAewa63sYPt","colab_type":"code","colab":{}},"source":["# word level tf-idf\n","tfidf_vectword = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1500)\n","# = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', max_features=1000)\n","tfidf_vectword.fit(df['preprocessed_tweet'])\n","#tfidf_vectchar.fit(df['preprocessed_tweet'])\n","xtrain_tfidf =  tfidf_vectword.transform(x)\n","#xtrain_tfidfchar = tfidf_vectchar.transform(df['tweet'])\n","#xvalid_tfidf =  tfidf_vect.transform(valid_x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvzRPDAJsYNC","colab_type":"code","colab":{}},"source":["##training the model\n","%%time\n","mb = MultinomialNB(alpha=0.08)\n","mb.fit(x_train1,y_train)\n","\n","y_pred = mb.predict(x_test_test)\n","print(mb.score(x_train1,y_train))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xy0x3aBcsYKk","colab_type":"code","colab":{}},"source":["submission = pd.DataFrame({\"Id\": [i+1 for i in range(len(y_pred))], \"Predicted\": y_pred})\n","pd.DataFrame(submission).to_csv(\"submission.csv\", index = None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aho0nux9sYIV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_wgyieWsYFh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}